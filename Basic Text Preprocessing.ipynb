{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727ddfd5",
   "metadata": {},
   "source": [
    "# Basic Text Preprocessing\n",
    "1. **Text Cleaning**\n",
    "    - Removing digits and words containing digits\n",
    "    - Removing newline characters and extra spaces\n",
    "    - Removing HTML tags\n",
    "    - Removing URLs\n",
    "    - Removing punctuations\n",
    "    \n",
    "\n",
    "2. **Basic Text Preprocessing**\n",
    "    - Case folding\n",
    "    - Expand contractions\n",
    "    - Chat word treatment\n",
    "    - Handle emojis\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Creating N-grams\n",
    "    - Stop words Removal\n",
    " \n",
    " \n",
    "3. **Advanced Preprocessing**\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - POS tagging\n",
    "    - NER\n",
    "    - Parsing\n",
    "    - Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b5733",
   "metadata": {},
   "source": [
    "#### Download and Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "886a451d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade pip\n",
    "!{sys.executable} -m pip install -q numpy pandas sklearn\n",
    "!{sys.executable} -m pip install -q nltk spacy gensim wordcloud textblob contractions text-clean unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4911df92",
   "metadata": {},
   "source": [
    "## Text Cleaning\n",
    "#### 1) Removing Digits and Words Containing Digits  \n",
    "- The **`re.sub(pattern, replacement_string, str)`** method return the string obtained by replacing the occurrences of `pattern` in `str` with the `replacement_string`. If the pattern isn‚Äôt found, the string is returned unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07733934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  a  string containing  words   having digits'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"This is abc32 a abc32xyz string containing 32abc words  32 having digits\"\n",
    "re.sub(r'\\w*\\d\\w*', '', mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55925b7b",
   "metadata": {},
   "source": [
    "#### 2) Removing Newline Characters and Extra Spaces \n",
    "- Most of the time text data contain extra spaces or while removing digits more than one space is left between the text.\n",
    "- We can use Python's string and re module to perform this pre-processing task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d025a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This is a string with lots of extra spaces in beteween words .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"      This         is a       string  with   lots of   extra spaces      in beteween    words     .\"\n",
    "re.sub(' +', ' ', mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5233f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      " This is\n",
      "a string\n",
      "with lots of new\n",
      "line characters.\n",
      "Preprocessed String: This is a string with lots of new line characters.\n"
     ]
    }
   ],
   "source": [
    "mystr = \"This is\\na string\\nwith lots of new\\nline characters.\"\n",
    "print('Original String:\\n', mystr)\n",
    "print('Preprocessed String:', re.sub('\\n',' ', mystr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd89973",
   "metadata": {},
   "source": [
    "#### 3) Removing HTML Tags\n",
    "- Once you get data via scraping websites, your data might contain HTML tags, which are not required as such in the data. So we need to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c6fbf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original String:\n",
      " <html> <head> An empty head. </head><body><p> This is so simple and fun. </p> </body> </html>\n",
      "Preprocessed Strings:     An empty head.     This is so simple and fun.      \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"<html> <head> An empty head. </head><body><p> This is so simple and fun. </p> </body> </html>\"\n",
    "print('Original String:\\n', mystr)\n",
    "print('Preprocessed Strings:', re.sub('<.+?>', ' ', mystr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68272c4c",
   "metadata": {},
   "source": [
    "#### 4) Removing URLS\n",
    "- At times the text data you have some URLS, which might not be helpful in suppose sentiment analysis. So better to remove those URLS from your dataset\n",
    "- Once again, we can use Python's re module to remove the URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9953a19a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good youTube lectures by Arif are available at  '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "mystr = \"Good youTube lectures by Arif are available at http://www.youtube.com/c/LearnWithArif/playlists\"\n",
    "re.sub('https?://(www/.)?\\w+\\S+', ' ', mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36062988",
   "metadata": {},
   "source": [
    "#### 5) Removing Punctuations\n",
    "- Punctuations are symbols that are used to divide written words into sentences and clauses\n",
    "- Once you tokenize your text, these punctuation symbols may become part of a token, and may become a token by itself, which is not required in most of the cases\n",
    "- We can use Python's `string.punctuation` constant and `replace()` method to replace any punctuation in text with an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a99dee68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "889e5275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A text having lot of s and puncutations\n"
     ]
    }
   ],
   "source": [
    "mystr = 'A {text} ^having$ \"lot\" of #s and [puncutations]!.;%..'\n",
    "newstr = ''.join([i for i in mystr if i not in string.punctuation])\n",
    "print(newstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028947bc",
   "metadata": {},
   "source": [
    "## Basic Text Preprocessing\n",
    "#### 1) Case Folding \n",
    "- The text we need to process may come in lower, upper, sentence, camel cases\n",
    "- If the text is in the same case, it is easy for a machine to interpret the words because the lower case and upper case are treated differently by the machine\n",
    "- In applications like Information Retrieval, we reduce all letters to lower case\n",
    "- In applications like sentiment analysis, machine translation and information extraction, keeping the case might be helpful. For example US vs us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d771475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is great series of lectures by arif at the deaprtment of ds'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"This IS GREAT series of Lectures by Arif at the Deaprtment of DS\"\n",
    "mystr.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3df5e",
   "metadata": {},
   "source": [
    "#### 2) Expand Contractions\n",
    "- Contractions are words or combinations of words that are shortened by dropping letters and replacing them by an apostrophe.\n",
    "- Examples:\n",
    "    - you're ---> you are\n",
    "    - ain't ---> am not / are not / is not / has not / have not\n",
    "    - you'll ---> you shall / you will\n",
    "    - wouldn't 've ---> would not haveyou are\n",
    "- In order to expand contractions, you can install and use the `contractions` module or can create your own dictionary to expand contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2caff05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dfc59812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you are\n",
      "are not\n",
      "you will\n",
      "would not have\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "print(contractions.fix(\"you're\"))\n",
    "print(contractions.fix(\"ain't\"))\n",
    "print(contractions.fix(\"you'll\"))\n",
    "print(contractions.fix(\"wouldn't've\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fec87c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \\nIt's awesome to meet new friends. We've been waiting for this day for so long.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = '''I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \n",
    "It's awesome to meet new friends. We've been waiting for this day for so long.'''\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "654e36e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be there within 5 min. Should not you be there too? I would love to see you there my dear. \n",
      "It is awesome to meet new friends. We have been waiting for this day for so long.\n"
     ]
    }
   ],
   "source": [
    "# using loop\n",
    "mylist = []\n",
    "for i in mystr.split(sep = ' '):\n",
    "    mylist.append(contractions.fix(i))\n",
    "mystr = ' '.join(mylist)\n",
    "print(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "70758cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will be there within 5 min. Should not you be there too? I would love to see you there my dear. It is awesome to meet new friends. We have been waiting for this day for so long.\n"
     ]
    }
   ],
   "source": [
    "# Using List Comprehensions\n",
    "\n",
    "mystr = '''I'll be there within 5 min. Shouldn't you be there too? I'd love to see u there my dear. \n",
    "It's awesome to meet new friends. We've been waiting for this day for so long.'''\n",
    "expanded_list = ' '.join([contractions.fix(i) for i in mystr.split()]) \n",
    "print(expanded_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d978a78",
   "metadata": {},
   "source": [
    "#### 3) Chat Word Treatment\n",
    "- Some commonly used abbreviated chat words that are used on social media these days are:\n",
    "    - GN for good night\n",
    "    - fyi for for your information\n",
    "    - asap for as soon as possible\n",
    "    - yolo for you only live once\n",
    "    - rofl for rolling on floor laughing\n",
    "    - nvm for never mind\n",
    "    - ofc for ofcourse\n",
    "\n",
    "- To pre-process any text containing such abbreviations we can search for an online dictionary, or can create a dictionary of our own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b4c0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 = { \n",
    "    'ack': 'acknowledge',\n",
    "    'omg': 'oh my God',\n",
    "    'aisi': 'as i see it',\n",
    "    'bi5': 'back in 5 minutes',\n",
    "    'lmk': 'let me know',\n",
    "    'gn' : 'good night',\n",
    "    'fyi': 'for your information',\n",
    "    'asap': 'as soon as possible',\n",
    "    'yolo': 'you only live once',\n",
    "    'rofl': 'rolling on floor laughing',\n",
    "    'nvm': 'never ming',\n",
    "    'ofc': 'ofcourse',\n",
    "    'blv' : 'boulevard',\n",
    "    'cir' : 'circle',\n",
    "    'hwy' : 'highway',\n",
    "    'ln' : 'lane',\n",
    "    'pt' : 'point',\n",
    "    'rd' : 'road',\n",
    "    'sq' : 'square',\n",
    "    'st' : 'street'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce959c6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'omg this is aisi I ack your work and will be bi5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"omg this is aisi I ack your work and will be bi5\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fbaaf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh my God this is as i see it I acknowledge your work and will be back in 5 minutes\n"
     ]
    }
   ],
   "source": [
    "newList = []\n",
    "for i in mystr.split(sep =' '):\n",
    "    if i in dict1.keys():\n",
    "        newList.append(dict1[i])\n",
    "    else:\n",
    "        newList.append(i)\n",
    "newString = ' '.join(newList)\n",
    "print(newString)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6582e8",
   "metadata": {},
   "source": [
    "#### 4) Handle Emojis\n",
    "- We come across lots and lots of emojis while scraping comments/posts from social media websites like Facebook, Instagram, Whatsapp, Twitter, LinkedIn, which needs to be removed from text.\n",
    "- Machine Learrning algorithm cannot understand emojis, so we have two options:\n",
    "    - Simply remove the emojis from the text data, and this can be done using `clean-text` library\n",
    "    - Replace the emoji with its meaning happy, sad, angry,....\n",
    ">- ***a) Remove Emojis***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d8e162a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'These emojis needs to be removed, there is a huge list...üòÉüò¨üòÇüòÖüòáüòâüòäüòúüòéü§óüôÑü§îüò°üò§üò≠ü§†ü§°ü§´üí©üòàüëªüôåüëç‚úåÔ∏èüëåüôè'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"These emojis needs to be removed, there is a huge list...üòÉüò¨üòÇüòÖüòáüòâüòäüòúüòéü§óüôÑü§îüò°üò§üò≠ü§†ü§°ü§´üí©üòàüëªüôåüëç‚úåÔ∏èüëåüôè\"\n",
    "mystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dcc47db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These emojis needs to be removed, there is a huge list...\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    " \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # code range for emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # code range for symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # code range for transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # code range for flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # code range for Dingbats\n",
    "        u\"\\U00002500-\\U00002BEF\"  # code range for chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\" \n",
    "        u\"\\u3030\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "print(emoji_pattern.sub(r'', mystr)) # no emoji\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dd67c5",
   "metadata": {},
   "source": [
    ">- ***b) Replace Emojis with Their Meanings***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34a74bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q  emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8774d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  :thumbs_up:'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "mystr = \"This is  üëç\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63e0fb2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am :thinking_face:'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"I am ü§î\"\n",
    "emoji.demojize(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c451b95f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is  positive'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr = \"This is  üëç\"\n",
    "emoji.replace_emoji(mystr, replace='positive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e8336b",
   "metadata": {},
   "source": [
    "#### 5) Spelling Correction\n",
    "- Most of the times the text data you have contain spelling errors, which if not corrected the same word may be represented in two or may be more different ways.\n",
    "- Almost all word editors, today underline incorrectly typed words and provide you possible correct options\n",
    "- So spelling correction is a two step task:\n",
    "    - Detection of spelling errors\n",
    "    - Correction of spelling errors\n",
    "        - Autocorrect as you type space\n",
    "        - Suggest a single correct word\n",
    "        - Suggest a list of words (from which you can choose one)\n",
    "- Types of spelling errors:\n",
    "    - **Non-word Errors:** are non-dictionary words or words that do not exist in the language dictionary. For example instead of typing `reading` the user typed `reeding`. These are easy to detect as they do not exist in the language dictionary and can be corrected using algorithms like shortest weighted edit distance and highest noisy channel probability.\n",
    "    - **Real-word Errors:** are dictionary words and are hard to detect. These can be of two types:\n",
    "        - Typographical errors: For example instead of typing `great` the user typed `greet`\n",
    "        - Cognitive errors (homophones: For example instead of typing `two` the user typed `too`\n",
    "\n",
    "\n",
    "<h4 align=\"left\" style=\"font-family:'Arial'\">\"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13683725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b88109a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.1'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import textblob\n",
    "textblob.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64279745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am reeding thiss gret boook on deta sciance suject, which is a greet curse\n",
      "<class 'textblob.blob.TextBlob'>\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "mystr = \"I am reeding thiss gret boook on deta sciance suject, which is a greet curse\"\n",
    "bloob = TextBlob(mystr)\n",
    "print(bloob)\n",
    "print(type(bloob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a4f229d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am reading this great book on data science subject, which is a greet curse'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bloob.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcf5e4",
   "metadata": {},
   "source": [
    ">-  The non-word errors like `reeding`, `this`, `gret`, `boook`, `deta`, `sciance` and `suject` have been corrected by `blob.correct()` method\n",
    ">- However, the real word errors like `greet` and `curse` are not corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce210b9",
   "metadata": {},
   "source": [
    "#### 6) Tokenization\n",
    "\n",
    "<img align=right src=\"images/tokenization.png\" width=\"500\">\n",
    "\n",
    "- **What is Tokenization:** Tokenization is a process of splitting text into meaningful segments called tokens. It can be character level, subword level, word level (unigram), two word level (bigram), three word level (trigram), and sentence level.\n",
    "- **Why to do Tokenization:** For classification of a product review as positive or negative, we may need to count the number of positive words and compare them with the count of negative words in the text of that review. For this we first need to tokenize the text of the product review. Tokens are the basic uilding locks of a document oject. Everything that helps us understand the meaning of the text is derived from tokens and their relationship to one another.\n",
    "- **How to do Tokenization:** In a sentence you may come across following four items:\n",
    "    -  **Prefix**:\tCharacter(s) at the beginning &#9656; `( ‚Äú $ Rs Dr`\n",
    "    -  **Suffix**:\tCharacter(s) at the end &#9656; `km ) , . ! ‚Äù`\n",
    "    -  **Infix**:\tCharacter(s) in between &#9656; `- -- / ...`\n",
    "    -  **Exception**: Special-case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied. From `L.A.!` the exclamation mark (!) is separated, while `L.A.` is not split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ac3da2",
   "metadata": {},
   "source": [
    "#### a) Tokenization using NLTK\n",
    "- NLTK stands for Natural Language Toolkit (https://www.nltk.org/). This is a suite of libraries and programs for statistical natural language processing for English language\n",
    "- NLTK was released in 2001, and is available for Windows, Mac OS X, and Linux.. \n",
    "- NLTK provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning.\n",
    "- NLTK fully supports the English language, but others like Spanish or French are not supported as extensively.\n",
    "- It is a string processing libbrary, i.e., you give a string as input and get a string as output\n",
    "- There are. different tokenizer available in nltk:\n",
    "    - `nltk.tokenize.sent_tokenize(str)` for sentence tokenization\n",
    "    - `nltk.tokenize.word_tokenize(str)` for word tokenization\n",
    "    - `nltk.tokenize.treebank.TreebankWordTokenizer(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a197456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2e9d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e9cc169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\iqbal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cf943ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'example', 'is', 'great', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "mystr=\"This example is great!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5539ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'should', 'do', 'your', 'Ph.D', 'in', 'A.I', '!']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"You should do your Ph.D in A.I!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78dfa043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'should', \"'ve\", 'sent', 'me', 'an', 'email', 'at', 'arif', '@', 'pucit.edu.pk', 'or', 'vist', 'http', ':', '//www/arifbutt.me']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"You should've sent me an email at arif@pucit.edu.pk or vist http://www/arifbutt.me\"\n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd69e4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Here', \"'s\", 'an', 'example', 'worth', '$', '100', '.', 'I', 'am', '384400km', 'away', 'from', 'earth', \"'s\", 'moon', '!']\n"
     ]
    }
   ],
   "source": [
    "mystr=\"Here's an example worth $100. I am 384400km away from earth's moon!\" \n",
    "print(word_tokenize(mystr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f6941e",
   "metadata": {},
   "source": [
    "#### b) Tokenization using spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- spaCy will isolate punctuation that does *not* form an integral part of a word. Quotation marks, commas, and punctuation at the end of a sentence will be assigned their own token. However, punctuation that exists as part of an email address, website or numerical value will be kept as part of the token.\n",
    "\n",
    "- **Download spacy model for English language**\n",
    "    - Spacy comes with pretrained models and pipelines for different languages.\n",
    "    - You can download any of the following models for English language, but better to download the small as this will require a reasonable amount of space on your disk, and may take a bit of time to download:\n",
    "        - en_core_web_sm\n",
    "        - en_core_web_md\n",
    "        - en_core_web_lg\n",
    "        - en_core_web_trf\n",
    "    - The model name consist of four parts:\n",
    "        - Language (en): The language abreviation can be `en` for English, `fr` for French, `zh` for Chinese\n",
    "        - Type (core/dep): It can be core for general-purpose pipeline with tagging, parsing, lemmatization and NER recognition. It can be dep for only tagging, parsing and lemmatization\n",
    "        - Genre (web/news): It measn the type of text the pipeline is trained on, e.g., web or news. \n",
    "        - Size: Package size indicator. `sm` for small, `md` for medium, `lg` for large and `trf for transformer\n",
    "        - Package version (a.b.c): Here a is the major version for spaCy, b is the minor version for spaCy, while c is the model verion dependent to the data on which the model is trained, it parameters, number of iterations and different vectors.\n",
    "        \n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79505154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d719710d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ab76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bb3db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 991.0 kB/s eta 0:00:13\n",
      "     --------------------------------------- 0.1/12.8 MB 919.0 kB/s eta 0:00:14\n",
      "     --------------------------------------- 0.1/12.8 MB 901.1 kB/s eta 0:00:15\n",
      "      -------------------------------------- 0.2/12.8 MB 958.1 kB/s eta 0:00:14\n",
      "      --------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "      --------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "      --------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "      -------------------------------------- 0.3/12.8 MB 684.6 kB/s eta 0:00:19\n",
      "      -------------------------------------- 0.3/12.8 MB 655.8 kB/s eta 0:00:20\n",
      "     - ------------------------------------- 0.4/12.8 MB 768.6 kB/s eta 0:00:17\n",
      "     - ------------------------------------- 0.5/12.8 MB 962.6 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.6/12.8 MB 1.0 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 0.6/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 0.7/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.2 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 0.8/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "     -- ------------------------------------- 0.9/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     -- ------------------------------------- 1.0/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     --- ------------------------------------ 1.1/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     --- ------------------------------------ 1.2/12.8 MB 1.3 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 1.4/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.6/12.8 MB 1.4 MB/s eta 0:00:08\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 1.6 MB/s eta 0:00:08\n",
      "     ------ --------------------------------- 2.0/12.8 MB 1.7 MB/s eta 0:00:07\n",
      "     ------ --------------------------------- 2.2/12.8 MB 1.8 MB/s eta 0:00:07\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     ------- -------------------------------- 2.4/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.6/12.8 MB 1.8 MB/s eta 0:00:06\n",
      "     -------- ------------------------------- 2.7/12.8 MB 1.9 MB/s eta 0:00:06\n",
      "     --------- ------------------------------ 3.0/12.8 MB 2.0 MB/s eta 0:00:05\n",
      "     --------- ------------------------------ 3.2/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.3/12.8 MB 2.1 MB/s eta 0:00:05\n",
      "     ---------- ----------------------------- 3.5/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.2 MB/s eta 0:00:05\n",
      "     ------------ --------------------------- 3.9/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.0/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ------------ --------------------------- 4.1/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.2/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     ------------- -------------------------- 4.3/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "     -------------- ------------------------- 4.6/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 4.9/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.0/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     --------------- ------------------------ 5.1/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 2.4 MB/s eta 0:00:04\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 2.4 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.1/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 2.5 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.5/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     -------------------- ------------------- 6.7/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     --------------------- ------------------ 6.9/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 7.3/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 2.6 MB/s eta 0:00:03\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.9/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.0/12.8 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.1/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.3/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.6/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.7/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------------------ --------- 9.8/12.8 MB 2.5 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 10.2/12.8 MB 2.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 10.4/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.6/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.5/12.8 MB 2.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 2.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\iqbal\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afac714",
   "metadata": {},
   "source": [
    "#### Example # 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f9619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' , A , 7 , km , Uber , cab , ride , from , Gulberg , to , Joher , Town , will , cost , you , $ , 20 , "
     ]
    }
   ],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "mystr=\"'A 7km Uber cab ride from Gulberg to Joher Town will cost you $20\" \n",
    "doc = nlp(mystr)\n",
    "for i in doc:\n",
    "    print(i, end = ' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a16dc",
   "metadata": {},
   "source": [
    "> <font color=green> Note that spacy has successfully tokenized the distance symbol, which nltk failed to separate.</font>\n",
    "\n",
    "#### Example # 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ae9cda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You , should , 've , sent , me , an , email , at , arif@pucit.edu.pk , or , vist , http://www , / , arifbutt.me , "
     ]
    }
   ],
   "source": [
    "# import spacy and load the language library\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "mystr=\"You should've sent me an email at arif@pucit.edu.pk or vist http://www/arifbutt.me\"\n",
    "doc = nlp(mystr)\n",
    "\n",
    "for token in doc:\n",
    "    print(token, end=' , ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18bae56",
   "metadata": {},
   "source": [
    ">- <font color=green> Note that spacy has kept the email as a single token, while nltk separated it.</font>\n",
    ">- <font color=green> However, spacy also failed to properly tokenize the URL :(</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8d122",
   "metadata": {},
   "source": [
    "#### 7) Creating N-grams\n",
    "- **What are n-grams?** \n",
    "    - A sequence of n words, can be bigram, trigram,....\n",
    "- **Why to use n-grams?** \n",
    "    - Capture contextual information (`good food` carries more meaning than just `good` and `food` when observed independently)\n",
    "    - Applications of N-grams:\n",
    "        - Sentence Completion\n",
    "        - Auto Spell Check and correction\n",
    "        - Auto Grammer Check and correction\n",
    "    - Is there a perfect value of n?\n",
    "        - Different types of n-grams are suitable for different types of applications. You should try different n-grams on your data in order to confidently conclude which one works the best among all for your text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "296c7952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal')\n",
      "('Iqbal', 'was')\n",
      "('was', 'a')\n",
      "('a', 'visionary')\n",
      "('visionary', 'philosopher')\n",
      "('philosopher', 'and')\n",
      "('and', 'politician')\n",
      "('politician', '.')\n",
      "('.', 'Thank')\n",
      "('Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mystr = \"Allama Iqbal was a visionary philosopher and politician. Thank you\"\n",
    "tokens = nltk.tokenize.word_tokenize(mystr)\n",
    "ngrams = nltk.ngrams(tokens, 2)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2916b278",
   "metadata": {},
   "source": [
    ">- The formula to calculate the count of n-grams in a document is: **`X - N + 1`**, where `X` is the number of words in a given document and `N` is the number of words in n-gram\n",
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 2 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 10\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c351f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was')\n",
      "('Iqbal', 'was', 'a')\n",
      "('was', 'a', 'visionary')\n",
      "('a', 'visionary', 'philosopher')\n",
      "('visionary', 'philosopher', 'and')\n",
      "('philosopher', 'and', 'politician')\n",
      "('and', 'politician', '.')\n",
      "('politician', '.', 'Thank')\n",
      "('.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "ngrams = nltk.ngrams(tokens, 3)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd703dd",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "    \\text{Count of N-grams} \\hspace{0.5cm} = \\hspace{0.5cm} 11 - 3 + 1 \\hspace{0.5cm} = \\hspace{0.5cm} 9\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c01728aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Allama', 'Iqbal', 'was', 'a')\n",
      "('Iqbal', 'was', 'a', 'visionary')\n",
      "('was', 'a', 'visionary', 'philosopher')\n",
      "('a', 'visionary', 'philosopher', 'and')\n",
      "('visionary', 'philosopher', 'and', 'politician')\n",
      "('philosopher', 'and', 'politician', '.')\n",
      "('and', 'politician', '.', 'Thank')\n",
      "('politician', '.', 'Thank', 'you')\n"
     ]
    }
   ],
   "source": [
    "ngrams = nltk.ngrams(tokens, 4)\n",
    "for grams in ngrams:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b82bb",
   "metadata": {},
   "source": [
    "#### 8) Stopwords Removal\n",
    "- Stopwords are extremely common words of a language having very little meanings, and it is usually safe to remove them and not consider them as important for later processing of our data.\n",
    "- Every language has its own set of stopwords. For example, some stopwords of English language are: the, a, an, was, were, at, will, on, in, from, to, me, you, yours,....\n",
    "- Whether you should remove stop words from your text or not mainly depends on the problem you are solving.\n",
    "- Remove stop words from your text if you are working on:\n",
    "    - Text Classification (Spam Filtering, Language Classification, Genre Classification)\n",
    "    - Caption Generation\n",
    "    - Auto-Tag Generation\n",
    "- Avoid removing stop words from your text if you are working on:\n",
    "    - Machine Translation\n",
    "    - Language Modeling\n",
    "    - Text Summarization\n",
    "    - Question-Answering problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb130c9",
   "metadata": {},
   "source": [
    "#### a) Using NLTK\n",
    "- The NLTK library has a defined set of stopwords for different languages like English. Here, we will focus on ‚Äòenglish‚Äô stopwords. One can also consider additional stopwords if required\n",
    "- Note that there is no single universal list of stopwords. The list of the stop words can change depending on your problem statement\n",
    "- Once you install nltk, it just install the base library and do not install all the packages related to different languages, different tokenization schemes, etc. To install all the nltk packages and corpora use `nltk.download()`\n",
    "- An installation window will pop up. Select all and click ‚ÄòDownload‚Äô to download and install the additional bundles. This will download all the dictionaries and other language and grammar data frames necessary for full NLTK functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96e1bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33793a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\iqbal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb64751",
   "metadata": {},
   "source": [
    "> After completion of downloading, you can load the package of `stopwords` from the `nltk.corpus` and use it to load the stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d434fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my', 'ourselves', 'be', 'now', 'why', 'any', \"needn't\", 'each', 'that', 'don', 'll', \"wasn't\", 'itself', 'isn', 'myself', 'being', 'him', 'hers', 'this', 'wouldn', 'our', 'ours', 'into', 'through', 'hadn', \"hasn't\", 'out', 'i', 'under', 'yourself', 'it', 'or', 'some', \"don't\", 'only', \"haven't\", 'shouldn', 'been', 'we', 'themselves', 'on', 'o', 'very', 'does', \"weren't\", \"couldn't\", 'up', \"isn't\", 'which', 'so', 'the', 'didn', 'theirs', 'where', 'his', \"doesn't\", 'of', 'few', 'for', 'ma', 'weren', 'yourselves', 'was', 'm', 'doesn', 'yours', 'from', 'were', 'they', \"shouldn't\", 'he', 'did', 'can', 'hasn', 'once', 'needn', 'her', 'are', 'while', \"won't\", 'those', 'and', 'these', \"mustn't\", 'all', 'here', 'as', 'by', 'because', 'until', 'no', 'am', 'off', 'will', \"you've\", 'them', 'again', 'having', 'himself', 'd', \"didn't\", 'you', 'then', 'down', 'not', 'couldn', 'below', 'should', 'than', 'too', 've', 'other', \"it's\", 'has', 'during', 'over', 'in', 'do', 'if', \"should've\", 'to', 'after', 'haven', 'she', 'mustn', 'more', 'is', 'whom', 'herself', 'me', 'with', 'what', \"you'd\", \"shan't\", 'about', \"mightn't\", 'a', 'an', 'aren', 'both', \"that'll\", 'most', 'nor', 'such', 'doing', 'own', \"wouldn't\", 'how', 'same', \"you'll\", 'won', 'had', 'wasn', 'against', 'have', 'its', 're', 's', \"aren't\", 'who', 'at', 'mightn', 'ain', 'before', 'shan', 'further', 'their', \"you're\", \"hadn't\", 'your', 'between', \"she's\", 'there', 'just', 'above', 'when', 't', 'y', 'but'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca49b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    newlist = list()\n",
    "    for word in text.split():\n",
    "        if word not in stopwords.words('english'):\n",
    "            newlist.append(word)\n",
    "    return ' '.join(newlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "034fd960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your Google account compromised. Your account closed. Immediately click link update account'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "mystr=\"Your Google account has been compromised. \\\n",
    "    Your account will be closed. Immediately click this link to update your account\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a19205c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie good'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mystr=\"This movie is not good\"\n",
    "remove_stopwords(mystr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef954f",
   "metadata": {},
   "source": [
    ">- <font color=green>For sentiment analysis purposes, the overall meaning of the resulting sentence is positive, which is not at all the reality. So either do not remove sentiment analysis while doing sentiment analysis or handle the negation before removing stopwords "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68de2a5",
   "metadata": {},
   "source": [
    "#### b) Using spaCy\n",
    "- **spaCy** (https://spacy.io/) is an open-source Natural Language Processing library designed to handle NLP tasks with the most efficient and state of the art algorithm, released in 2015. \n",
    "- Spacy support many languages (over 65) where you can perform tokenizing, however, for this other than importing spacy, you have to load the appropriate library using spacy.load() method. But before that make sure you have downloaded the model in your system.\n",
    "- **Download spacy model for English language:** Spacy comes with pretrained models and pipelines for different languages. We have already downloaded the pre-trained spacy model for English language\n",
    "> For details read spaCy101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d4a7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d9b8505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n",
      "{'thru', 'although', '‚Äôd', 'various', 'thereby', 'due', 'him', 'hundred', 'per', 'ours', 'through', 'bottom', 'i', 'or', 'only', 'been', 'front', 'fifteen', 'become', 'regarding', 'give', 'hereafter', 'very', 'whether', 'up', 'which', 'somewhere', 'must', 'his', 'us', 'yet', 'may', 'yourselves', 'therein', '‚Äôll', 'are', 'her', 'herein', 'whose', '‚Äòll', 'by', 'no', 'off', 'them', 'himself', 'ten', 'thereafter', 'none', 'though', 'latter', 'noone', 'something', 'in', '‚Äôs', 'n‚Äòt', 'perhaps', 'already', 'amount', 'several', 'formerly', 'together', 'about', 'otherwise', 'beside', 'third', 'nor', 'well', 'same', 'whoever', 're', '‚Äòs', '‚Äôve', 'seemed', 'twelve', 'before', 'eight', 'everyone', 'becoming', 'when', 'my', 'thereupon', 'why', 'that', 'also', 'sometime', 'call', 'except', 'some', 'themselves', 'whereby', 'put', 'does', 'nowhere', 'another', 'the', 'hence', 'ever', 'ca', 'nine', 'mine', 'anyone', 'take', 'few', 'afterwards', 'of', 'from', 'were', 'they', 'go', 'can', 'once', 'while', 'via', 'all', 'behind', 'because', 'whereupon', 'beyond', 'whole', 'you', 'not', 'should', 'might', 'too', 'four', 'former', 'always', 'alone', 'onto', 'to', 'more', 'whatever', 'me', 'least', 'eleven', 'moreover', 'an', 'nevertheless', 'others', 'three', 'most', 'own', 'doing', 'such', 'really', 'along', 'without', 'every', 'against', 'twenty', 'anything', 'first', 'their', 'done', 'please', 'but', 'anywhere', 'ourselves', 'top', 'keep', 'any', 'used', 'each', 'would', 'itself', 'elsewhere', 'hers', 'this', 'n‚Äôt', 'our', 'never', 'it', 'sometimes', 'full', 'thus', 'seem', 'whereafter', 'wherever', 'beforehand', 'where', 'less', 'forty', 'for', 'was', 'yours', 'see', 'often', 'say', 'rather', 'and', 'these', 'here', 'as', 'will', 'am', 'amongst', 'much', 'down', 'move', 'below', 'make', \"'re\", 'empty', 'became', 'other', 'has', 'everything', 'do', 'made', 'after', 'seems', 'one', 'whereas', 'she', 'is', 'whom', 'get', 'name', 'with', 'what', 'mostly', 'quite', 'both', 'last', 'nobody', 'throughout', '‚Äôre', 'else', \"'ll\", 'latterly', 'using', 'cannot', 'towards', 'anyhow', 'across', 'its', 'two', 'among', 'therefore', 'who', 'upon', 'indeed', 'next', \"'d\", 'still', 'now', 'be', 'show', '‚Äòm', 'myself', 'side', 'being', '‚Äòd', 'besides', 'into', 'sixty', 'out', 'under', 'yourself', 'we', 'on', 'could', 'so', 'whither', 'even', 'someone', 'many', 'namely', 'he', 'did', 'either', 'within', 'neither', 'five', 'those', '‚Äòve', 'since', 'until', 'again', '‚Äôm', 'unless', 'thence', 'then', 'becomes', 'than', 'during', 'over', 'nothing', 'enough', 'if', 'fifty', 'almost', 'however', \"'ve\", 'toward', 'herself', 'everywhere', \"'m\", 'a', 'part', 'back', 'six', 'hereupon', 'serious', 'how', 'anyway', 'had', 'meanwhile', 'have', 'around', \"n't\", 'hereby', 'at', 'further', 'wherein', 'seeming', 'your', 'between', 'there', '‚Äòre', 'whenever', 'just', 'whence', 'above', \"'s\", 'somehow'}\n"
     ]
    }
   ],
   "source": [
    "# returns a set of around 326 English stopwords built into spaCy\n",
    "print(len(nlp.Defaults.stop_words))\n",
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf708af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_spacy(text):\n",
    "    new_text = list()\n",
    "    for word in text.split():\n",
    "        if word not in nlp.Defaults.stop_words:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
