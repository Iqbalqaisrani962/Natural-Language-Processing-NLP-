{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfc303d9",
   "metadata": {},
   "source": [
    "# Advanced Text Preprocessing\n",
    "1. **Text Cleaning**\n",
    "    - Removing digits and words containing digits\n",
    "    - Removing newline characters and extra spaces\n",
    "    - Removing HTML tags\n",
    "    - Removing URLs\n",
    "    - Removing punctuations\n",
    "    \n",
    "\n",
    "2. **Basic Text Preprocessing**\n",
    "    - Case folding\n",
    "    - Expand contractions\n",
    "    - Chat word treatment\n",
    "    - Handle emojis\n",
    "    - Spelling correction\n",
    "    - Tokenization\n",
    "    - Creating N-grams\n",
    "    - Stop words Removal\n",
    " \n",
    " \n",
    "3. **Advanced Preprocessing**\n",
    "    - Stemming\n",
    "    - Lemmatization\n",
    "    - POS tagging\n",
    "    - NER\n",
    "    - Parsing\n",
    "    - Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb405f3",
   "metadata": {},
   "source": [
    "## 1) Stemming\n",
    "- ***Word Normalization***\n",
    "     - Case Folding\n",
    "     - Stemming\n",
    "     - Lematization\n",
    "     >- ***Examples*** \n",
    "          - Consult      --> Consult\n",
    "          - Consultant   --> Consult\n",
    "          - COnsultants  --> Consult\n",
    "          - Consulting   --> Consult\n",
    "          - Consultative --> Consult"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ba64de",
   "metadata": {},
   "source": [
    "- By filtering multiple words to their root words, the distinct count of unique words get reduced without affecting the meaning of the sentence. This has a good affect on the performance of the Machine Learning Algorithms\n",
    "- Both stemming and lematization aims to reduce terms to their stems and are heavily used in information retrival\n",
    "- ***Stemming*** algorithms used fixed rules such as cutting the prefix/suffix to drive the base/root word. It do so even if the stem itself is not a valid word in the language. It is faster as it cuts the words without knowing the context.\n",
    ">- ***Examples***\n",
    "      - Studies --> Studi\n",
    "      - Cries   --> Cri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df41af53",
   "metadata": {},
   "source": [
    "***Stemming Algorithms***\n",
    "1) Porter Stemmer\n",
    "2) Snowball Stemmer\n",
    "3) Lancaster Stemmer\n",
    "4) Regex-based Stemmer\n",
    "- ***Downloading & Installing the Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4077bba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -q --upgrade pip\n",
    "!{sys.executable} -m pip install -q numpy pandas sklearn\n",
    "!{sys.executable} -m pip install -q nltk spacy gensim wordcloud textblob contractions text-clean unicode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239bccdc",
   "metadata": {},
   "source": [
    "### 1) Stemming Using the NLTK's `Porter Stemmer`\n",
    "- One of the most common and effective stemming tolls is Porters Algorithm developed by the Martin Porter in 1980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85dd23e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MARTIN_EXTENSIONS', 'NLTK_EXTENSIONS', 'ORIGINAL_ALGORITHM', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_apply_rule_list', '_contains_vowel', '_ends_cvc', '_ends_double_consonant', '_has_positive_measure', '_is_consonant', '_measure', '_replace_suffix', '_step1a', '_step1b', '_step1c', '_step2', '_step3', '_step4', '_step5a', '_step5b', 'mode', 'pool', 'stem', 'vowels']\n"
     ]
    }
   ],
   "source": [
    "# Import the toolkit and the PorterStemmer() method from the stem module\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.regexp import RegexpStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(dir(ps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "502ec94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'poni'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps._step1a('ponies')\n",
    "# Removes 'es' from the last in step1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd3ecfa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cutting'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps._step1a('cutting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c22f48d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cut'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps._step1b('cutting')\n",
    "# In the step1b, chop 'ing' from the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6512f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait  ---->  wait\n",
      "waiting  ---->  wait\n",
      "waits  ---->  wait\n",
      "waited  ---->  wait\n",
      "consult  ---->  consult\n",
      ",  ---->  ,\n",
      "consultative  ---->  consult\n",
      ",  ---->  ,\n",
      "consulting  ---->  consult\n",
      ",  ---->  ,\n",
      "consultation  ---->  consult\n",
      "university  ---->  univers\n",
      ",  ---->  ,\n",
      "universe  ---->  univers\n",
      ",  ---->  ,\n",
      "universal  ---->  univers\n",
      "studies  ---->  studi\n",
      ",  ---->  ,\n",
      "cry  ---->  cri\n",
      ",  ---->  ,\n",
      "cries  ---->  cri\n",
      "bicycle  ---->  bicycl\n",
      ",  ---->  ,\n",
      "USA  ---->  usa\n",
      "data  ---->  data\n",
      "datum  ---->  datum\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "mystr = 'Wait waiting waits waited \\\n",
    "         consult, consultative, consulting, consultation\\\n",
    "         university, universe, universal\\\n",
    "         studies, cry , cries bicycle, USA\\\n",
    "          data datum'\n",
    "ps = PorterStemmer()\n",
    "for i in word_tokenize(mystr):\n",
    "    root_words = ps.stem(i)\n",
    "    print(i, ' ----> ', root_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f278023",
   "metadata": {},
   "source": [
    ">- ***Over Stemming:*** In over stemming too much of the data is cut off (i.e studies --> studi) or two words of different stems map to same word (i.e university, universal and universe are all reduced worngly to the same stem universe)\n",
    ">- ***Under Stemming:*** It is just the opposite of the over stemming, in which two words of the same stem  are mapped to the different stems (the stem of both the data and datum is data, but are reduced wrongly to different stems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfaede6",
   "metadata": {},
   "source": [
    "### 2) Lemmatization Using NLTK's `WordNetLemmatizer`\n",
    "- ***Stemming*** algorithms use fix rules such as cutting the prefix/suffix to drive the base/root word. It do so even if the stem itself is not valid word in the language. It is faster as it cuts the words without knowing the context\n",
    "- It is rule-based approach \n",
    "- When we convert any word into root-form then the stemming may create the non-existence meaning of the word\n",
    "- Stemming is preferred when meaning of the words are not important in analysis\n",
    "- For example: \"Studies\"-->\"Studi:\n",
    "- ***Lemmatization*** use knowledge of language to drive the base/root word also known as lemma. Lemmatization ensures that the root word (lemma) belongs to a language. Since lemmatization involves the meaning of the word from something like dictionary, it's time comsuming. \n",
    "- It is dictionary-based approach\n",
    "- It always gives the dictionary word while converting to root form\n",
    "- - Lemmatization is preferred when meaning of the words are important in analysis\n",
    "- For example: Question Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50cf78cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wait --> Wait\n",
      "waiting --> waiting\n",
      "waits --> wait\n",
      "waited --> waited\n",
      "consult --> consult\n",
      ", --> ,\n",
      "consultative --> consultative\n",
      ", --> ,\n",
      "consulting --> consulting\n",
      ", --> ,\n",
      "consultation --> consultation\n",
      "university --> university\n",
      ", --> ,\n",
      "universe --> universe\n",
      ", --> ,\n",
      "universal --> universal\n",
      "studies --> study\n",
      ", --> ,\n",
      "cry --> cry\n",
      ", --> ,\n",
      "cries --> cry\n",
      "bicycle --> bicycle\n",
      ", --> ,\n",
      "USA --> USA\n",
      "data --> data\n",
      "datum --> datum\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "mystr = 'Wait waiting waits waited \\\n",
    "         consult, consultative, consulting, consultation\\\n",
    "         university, universe, universal\\\n",
    "         studies, cry , cries bicycle, USA\\\n",
    "          data datum'\n",
    "for i in nltk.word_tokenize(mystr):\n",
    "    print(i, '-->', lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037afa20",
   "metadata": {},
   "source": [
    "### 3) POS tagging\n",
    "- Part of speech (POS) tagging is a process of assigning a part-of-speech to each word in the text\n",
    "- POS tagging actually a classification problem where each word in the text is assigned a proper Part of Speech\n",
    "- These are the ten parts of speech (POS)\n",
    ">- Noun\n",
    ">- Pronoun\n",
    ">- Verb\n",
    ">- Adverb\n",
    ">- Articles\n",
    ">- Adjective\n",
    ">- Punctuations\n",
    ">- Interjections\n",
    ">- Conjunctions\n",
    ">- Numeral\n",
    "- POS are divided into two broad categories\n",
    "> 1) Closed Class Type: (Prepositions, Articles, Pronouns)\n",
    "> 2) Open Class Type: (Nouns, Verbs, Adjectives, Adverbs)\n",
    "- These are the three types of POS tagging:\n",
    "> 1) Rule based POS tagging (E-Brill's Tagger)\n",
    "> 2) Stochastic POS tagging (Hidden Markov Model and Viterbi Algo)\n",
    "> 3) Transformation based POS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da910",
   "metadata": {},
   "source": [
    "#### POS tagging using Spacy\n",
    "- POS tagging in Spacy library is an easy task. We just instantiate a spacy object as doc. We iterate over tokens of `spacy doc` object and use `pos_` and `tag_` attributes to print the coarse-grained POS tag. Spacy also lets you to acccess the detailed explanation of these POS tags by using `spacy.explain()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99be6abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the english library of the spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75cf9dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a pre-trained model, let us check the pipes of the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6608348",
   "metadata": {},
   "source": [
    ">- A pipe is an individual component of a pipeline\n",
    ">- In the case of spacy, there are few different pipes that perform different tasks. The tokenizer, tokenizes the text into individual tokens, the parser parses the text, and the NER identifies entities and labels then accordingly. All of this data is stored in the Doc object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea7347d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "mytext = \"The quick brown box name is Zoro and it high jumped over the lazy dog's back\"\n",
    "Doc = nlp(mytext)\n",
    "print(type(Doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bcc966c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The quick brown box name is Zoro and it high jumped over the lazy dog's back"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "519df792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '_bulk_merge', '_context', '_get_array_attrs', '_realloc', '_vector', '_vector_norm', 'cats', 'char_span', 'copy', 'count_by', 'doc', 'ents', 'extend_tensor', 'from_array', 'from_bytes', 'from_dict', 'from_disk', 'from_docs', 'from_json', 'get_extension', 'get_lca_matrix', 'has_annotation', 'has_extension', 'has_unknown_spaces', 'has_vector', 'is_nered', 'is_parsed', 'is_sentenced', 'is_tagged', 'lang', 'lang_', 'mem', 'noun_chunks', 'noun_chunks_iterator', 'remove_extension', 'retokenize', 'sentiment', 'sents', 'set_ents', 'set_extension', 'similarity', 'spans', 'tensor', 'text', 'text_with_ws', 'to_array', 'to_bytes', 'to_dict', 'to_disk', 'to_json', 'to_utf8_array', 'user_data', 'user_hooks', 'user_span_hooks', 'user_token_hooks', 'vector', 'vector_norm', 'vocab']\n"
     ]
    }
   ],
   "source": [
    "# Doc level attributes\n",
    "print(dir(Doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21ead80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', '__bytes__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__pyx_vtable__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', 'ancestors', 'check_flag', 'children', 'cluster', 'conjuncts', 'dep', 'dep_', 'doc', 'ent_id', 'ent_id_', 'ent_iob', 'ent_iob_', 'ent_kb_id', 'ent_kb_id_', 'ent_type', 'ent_type_', 'get_extension', 'has_dep', 'has_extension', 'has_head', 'has_morph', 'has_vector', 'head', 'i', 'idx', 'iob_strings', 'is_alpha', 'is_ancestor', 'is_ascii', 'is_bracket', 'is_currency', 'is_digit', 'is_left_punct', 'is_lower', 'is_oov', 'is_punct', 'is_quote', 'is_right_punct', 'is_sent_end', 'is_sent_start', 'is_space', 'is_stop', 'is_title', 'is_upper', 'lang', 'lang_', 'left_edge', 'lefts', 'lemma', 'lemma_', 'lex', 'lex_id', 'like_email', 'like_num', 'like_url', 'lower', 'lower_', 'morph', 'n_lefts', 'n_rights', 'nbor', 'norm', 'norm_', 'orth', 'orth_', 'pos', 'pos_', 'prefix', 'prefix_', 'prob', 'rank', 'remove_extension', 'right_edge', 'rights', 'sent', 'sent_start', 'sentiment', 'set_extension', 'set_morph', 'shape', 'shape_', 'similarity', 'subtree', 'suffix', 'suffix_', 'tag', 'tag_', 'tensor', 'text', 'text_with_ws', 'vector', 'vector_norm', 'vocab', 'whitespace_']\n"
     ]
    }
   ],
   "source": [
    "# Token level attributes\n",
    "print(dir(Doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef093f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Token     Course POS TagFined Graned POS Tag Explanation\n",
      "\u001b[m\n",
      "The       DET       DT                   determiner\n",
      "quick     ADJ       JJ                   adjective (English), other noun-modifier (Chinese)\n",
      "brown     PROPN     NNP                  noun, proper singular\n",
      "box       NOUN      NN                   noun, singular or mass\n",
      "name      NOUN      NN                   noun, singular or mass\n",
      "is        AUX       VBZ                  verb, 3rd person singular present\n",
      "Zoro      PROPN     NNP                  noun, proper singular\n",
      "and       CCONJ     CC                   conjunction, coordinating\n",
      "it        PRON      PRP                  pronoun, personal\n",
      "high      ADV       RB                   adverb\n",
      "jumped    VERB      VBD                  verb, past tense\n",
      "over      ADP       IN                   conjunction, subordinating or preposition\n",
      "the       DET       DT                   determiner\n",
      "lazy      ADJ       JJ                   adjective (English), other noun-modifier (Chinese)\n",
      "dog       NOUN      NN                   noun, singular or mass\n",
      "'s        PART      POS                  possessive ending\n",
      "back      NOUN      NN                   noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m')\n",
    "print(f'{\"Token\":{10}}{\"Course POS Tag\":{10}}{\"Fined Graned POS Tag\":{21}}{\"Explanation\"}')\n",
    "print('\\033[m')\n",
    "for token in Doc:\n",
    "    print(f'{token.text:{10}}{token.pos_:{10}}{token.tag_:{21}}{spacy.explain(token.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0a2b23",
   "metadata": {},
   "source": [
    ">- To view the coarse POS tag use `token.pos_`\n",
    ">- To view the fine_graned POS tag use `tag.tag_`\n",
    ">- To view the description of either type of tag use `spacy.explain(tag)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad35280b",
   "metadata": {},
   "source": [
    "***Example #02***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b3c4306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make ---> VERB VB verb, base form\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('I am going to make dinner')\n",
    "word = doc[4]\n",
    "print(word, '--->', word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8aad0cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make ---> NOUN NN noun, singular or mass\n"
     ]
    }
   ],
   "source": [
    "doc = nlp('What is the make of your laptop?')\n",
    "word = doc[3]\n",
    "print(word, '--->', word.pos_, word.tag_, spacy.explain(word.tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8237b99",
   "metadata": {},
   "source": [
    "#### Visualizing Parts of Speech Using `displacy`\n",
    "- To visually render POS, we can use `displacy` module of the spacy\n",
    "- On Jupyter Notebook: `displacy.render()`\n",
    "- On other IDEs: `displacy.serve()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2448dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7390d8e5d5054772862cdee2a937c4d5-0\" class=\"displacy\" width=\"750\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Arif</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">playing</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Cricket</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7390d8e5d5054772862cdee2a937c4d5-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,2.0 400.0,2.0 400.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7390d8e5d5054772862cdee2a937c4d5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7390d8e5d5054772862cdee2a937c4d5-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7390d8e5d5054772862cdee2a937c4d5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-7390d8e5d5054772862cdee2a937c4d5-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-7390d8e5d5054772862cdee2a937c4d5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp('Arif is playing Cricket')\n",
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ca2b2a",
   "metadata": {},
   "source": [
    "### 4) Named Entity Recognition (NER)\n",
    "- An ***Entity*** is a common thing that belongs to a noun family. It can be subject or onject\n",
    "- A ***Named Entity*** refers to a real world or conceptual object that can be represented by a proper noun\n",
    "- ***Named Entity Recognition (NER)*** is a subtask of `Information Extraction` that seeks to locate and classify named entities mentioned in unstructed text into predefined categories such as person names, organizations, locations, medical codes, date time expressions, monetary values, percentages etc.\n",
    "- It is also known as entity identification or entity chunking or entity extraction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a687165",
   "metadata": {},
   "source": [
    "***Example:*** Arif (`person`) has moved to Karachi (`GPE`) where he will be playing (`event`) basketball (`product`) on 26 Dec 2023 (`date`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541d55d",
   "metadata": {},
   "source": [
    "***Use Cases of NER***\n",
    "- 1) Coreference Resolution\n",
    "- 2) Information Extraction\n",
    "- 3) Search Engines\n",
    "- 4) Recommendation Systems\n",
    "- 5) Questions Answers systems and chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdcdd6",
   "metadata": {},
   "source": [
    "***How to NER***\n",
    "- Dictionary-Based Approach\n",
    "- Rule-Based Approach\n",
    "- Machine Learning-Based Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5330dcd7",
   "metadata": {},
   "source": [
    "#### NER using spacy\n",
    "***Example # 01***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25935672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the english library of the spacy\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6833eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is a pre-trained model, let us check the pipes of the pipeline\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "742a2588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CARDINAL',\n",
       " 'DATE',\n",
       " 'EVENT',\n",
       " 'FAC',\n",
       " 'GPE',\n",
       " 'LANGUAGE',\n",
       " 'LAW',\n",
       " 'LOC',\n",
       " 'MONEY',\n",
       " 'NORP',\n",
       " 'ORDINAL',\n",
       " 'ORG',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'PRODUCT',\n",
       " 'QUANTITY',\n",
       " 'TIME',\n",
       " 'WORK_OF_ART']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The pre-trained model support following named entities\n",
    "nlp.pipe_labels['ner']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97c07819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punjab University ---> ORG\n",
      "2022 ---> DATE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Iqbal works full time in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Punjab University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2022\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " and part time in twitter</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mystr = 'Iqbal works full time in Punjab University since 2022 and part time in twitter'\n",
    "doc = nlp(mystr)\n",
    "for i in doc.ents:\n",
    "    print(i.text, '--->', i.label_)\n",
    "# Visualization of NET\n",
    "displacy.render(doc, style = 'ent', jupyter = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e55643",
   "metadata": {},
   "source": [
    "### 5) Corefernce Resolution\n",
    "- Coreference is a process of identify all noun phrases that refer to the same entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e77392",
   "metadata": {},
   "source": [
    "***Use Cases of Coreference Resolution***\n",
    "- 1) Coreference Resolution\n",
    "- 2) Information Retrival\n",
    "- 3) Text Summarization\n",
    "- 4) Machine Translation\n",
    "- 5) Question Answering System "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
